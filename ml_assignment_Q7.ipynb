{"cells":[{"cell_type":"code","execution_count":78,"metadata":{"id":"2-ZYhG2zn3qf"},"outputs":[],"source":["%%capture\n","!pip install wget"]},{"cell_type":"code","execution_count":79,"metadata":{"id":"IcqEP3fGkec9","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","# Please add other necessary imports here\n","from nltk.tokenize import RegexpTokenizer\n","import re\n","\n","# Please add necessary imports here\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","import nltk\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import LinearSVC\n","from sklearn.calibration import CalibratedClassifierCV"]},{"cell_type":"code","execution_count":80,"metadata":{"id":"Nq7O9NKnnorw"},"outputs":[],"source":["import wget\n","from pathlib import Path\n","filename = wget.download(\"https://github.com/MIE451-1513-2023/course-datasets/raw/main/20_newsgroups.zip\", \"20_newsgroups.zip\")\n","_ = wget.download(\"https://github.com/MIE451-1513-2023/course-datasets/raw/main/training_files_Q7.txt\", \"training_files_Q7.txt\")\n","_ = wget.download(\"https://github.com/MIE451-1513-2023/course-datasets/raw/main/testing_files_Q7.txt\", \"testing_files_Q7.txt\")"]},{"cell_type":"code","execution_count":81,"metadata":{"id":"uwVs1bGK64FB"},"outputs":[],"source":["%%capture\n","!unzip 20_newsgroups.zip"]},{"cell_type":"code","execution_count":82,"metadata":{"id":"QgyLW2Lp69RD"},"outputs":[],"source":["DATA_DIR = \"20_newsgroups\"\n","ALL_FILES = [pth for pth in Path(DATA_DIR).glob(\"**/*\") if pth.is_file() and not pth.name.startswith(\".\")]"]},{"cell_type":"markdown","metadata":{"id":"432hrnVOkedA"},"source":["# Q7"]},{"cell_type":"markdown","metadata":{"id":"PSihMe6ekedB"},"source":["## Q7(a)"]},{"cell_type":"markdown","metadata":{"id":"HwX8PxjCkedD"},"source":["use the following code cell to implement your feature encoding"]},{"cell_type":"code","execution_count":83,"metadata":{"id":"aknEVgx0kedD"},"outputs":[],"source":["def data_q7(file_list, num_words=1000):\n","    def clean_file_text(text):\n","        new_text = re.sub(\"Newsgroups:.*?\\n\", \"\", text)\n","        new_text = re.sub(\"Xref:.*?\\n\", \"\", new_text)\n","        new_text = re.sub(\"Path:.*?\\n\", \"\", new_text)\n","        new_text = re.sub(\"Date:.*?\\n\", \"\", new_text)\n","        new_text = re.sub(\"Followup-To:.*?\\n\", \"\", new_text)\n","        new_text = re.sub(\"Lines:.*?\\n\", \"\", new_text)\n","        new_text = re.sub(\"Reply-To:.*?\\n\", \"\", new_text)\n","        new_text = re.sub(\"Message-ID:.*?\\n\", \"\", new_text)\n","        new_text = re.sub(\"From:.*?\\n\", \"\", new_text)\n","        new_text = re.sub(\"NNTP-Posting-Host:.*?\\n\", \"\", new_text)\n","        return new_text\n","    \n","    def get_topic_name(file_path):\n","        return file_path.parent.name\n","\n","    def get_target(topic_name):\n","        topics = [\"talk.politics.mideast\", \"rec.autos\", \"comp.sys.mac.hardware\", \"alt.atheism\", \"rec.sport.baseball\", \n","        \"comp.os.ms-windows.misc\", \"rec.sport.hockey\", \"sci.crypt\", \"sci.med\", \"talk.politics.misc\", \n","        \"rec.motorcycles\", \"comp.windows.x\", \"comp.graphics\", \"comp.sys.ibm.pc.hardware\", \"sci.electronics\",\n","        \"talk.politics.guns\", \"sci.space\", \"soc.religion.christian\", \"misc.forsale\", \"talk.religion.misc\"]\n","        return topics.index(topic_name)\n","    \n","    class CustomTokenizer:\n","        def __init__(self):\n","            nltk.download('wordnet')\n","            nltk.download('stopwords')\n","            self.wnl = WordNetLemmatizer()\n","            self.stop_words = set(stopwords.words('english'))\n","            self.tokenizer = RegexpTokenizer(r\"\\w+\")\n","        def __call__(self, doc):\n","            return [self.wnl.lemmatize(word.lower()) for word in self.tokenizer.tokenize(doc) if word.lower() not in self.stop_words]\n","    corpus = []\n","    for file_path in ALL_FILES:\n","        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n","            file_data = file.read()\n","            file_data = clean_file_text(file_data)\n","            corpus.append(file_data)\n","    vectorizer = TfidfVectorizer(tokenizer=CustomTokenizer(),ngram_range=(1, 3), min_df=7)\n","    X = vectorizer.fit_transform(corpus)\n","    X = pd.DataFrame.sparse.from_spmatrix(X, index = [str(f).replace('\\\\','/') for f in ALL_FILES], columns=vectorizer.get_feature_names_out())#.replace('\\\\','/')\n","    y = [get_target(get_topic_name(file_path)) for file_path in ALL_FILES]\n","    # validate return types\n","    assert isinstance(X, pd.DataFrame) and isinstance(y, list), \"incorrect return types\"\n","    \n","    return X, y"]},{"cell_type":"markdown","metadata":{"id":"FNRE4K4gkedE"},"source":["## Q7(b)"]},{"cell_type":"markdown","metadata":{"id":"AzXn77E0kedE"},"source":["Use the following code cell to implement your model"]},{"cell_type":"code","execution_count":84,"metadata":{"id":"gY3zH_yfkedE"},"outputs":[],"source":["def build_model_q7():\n","    # Write your code here, define your model and return it\n","    MODELQ7 = CalibratedClassifierCV(estimator=LinearSVC(C=0.7), cv=2)\n","    return MODELQ7"]},{"cell_type":"markdown","metadata":{"id":"MWKu6i31kedF"},"source":["Code for evaluating p at k "]},{"cell_type":"code","execution_count":85,"metadata":{"id":"s4o8CXDJkedF"},"outputs":[],"source":["def calculate_average_precision_at_k(model_q7, data_func, all_files, training_files, testing_files, k=None):\n","  \n","    training_files = [str(f) for f in open(training_files, mode='r').read().splitlines()]\n","    testing_files = [str(f) for f in open(testing_files, mode='r').read().splitlines()]\n","    if k is None:\n","        k = len(testing_files)\n","\n","    X, y = data_func(all_files)\n","    X[\"gt\"] = y\n","    training = X.loc[training_files]\n","    X_train = training.loc[:, training.columns!=\"gt\"]\n","    y_train = training[\"gt\"].values\n","\n","    testing = X.loc[testing_files]\n","    X_test = testing.loc[:, testing.columns!=\"gt\"]\n","    y_test = testing[\"gt\"].values\n","\n","    model_q7.fit(X_train, y_train)\n","    y_pred = model_q7.predict(X_test)\n","    y_pred_prob = model_q7.predict_proba(X_test)\n","    confidences = np.max(y_pred_prob, axis=1)\n","    \n","    p_at_k = []\n","    rel_at_k = []\n","    confidence_order = np.argsort(confidences)\n","    for i in range(1, k+1):\n","        top_confidence = confidence_order[-i:]\n","        pred_top_i = y_pred[top_confidence]\n","        gt_top_i = np.array(y_test)[top_confidence]\n","        p_at_i = np.sum(pred_top_i == gt_top_i) / i\n","        rel_at_i = (pred_top_i[0] == gt_top_i[0])\n","        p_at_k.append(p_at_i)\n","        rel_at_k.append(rel_at_i)\n","    print(f\"average precision at {k} is {np.dot(p_at_k, rel_at_k) / k}\")\n","    return np.dot(p_at_k, rel_at_k) / k"]},{"cell_type":"code","execution_count":86,"metadata":{"id":"QmicMF0ykedG"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\kirby\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\kirby\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","c:\\Users\\kirby\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["average precision at 4000 is 0.8600042117237919\n"]}],"source":["# Example usage:\n","######This line of code must be able to run on Google Colab in under 7 minutes.#####\n","######Code that runs longer than 7 minutes on the autograder will receive 0 marks for Q7#####\n","m = calculate_average_precision_at_k(build_model_q7(), data_q7, ALL_FILES, \"training_files_Q7.txt\", \"testing_files_Q7.txt\")"]},{"cell_type":"markdown","metadata":{"id":"J8y4vOO5kedH"},"source":["# Q7(c)"]},{"cell_type":"markdown","metadata":{"id":"jXORTLuikedH"},"source":["I encoded the documents using TF-IDF, applying lowercase, stop word filtering and lemmatizer as processing. I selected unigrams, bigrams and trigrams which have a min document frequency no less than 7 (selected to reduce computation time)\n","\n","I used a support vector classifier (SVC), wrapped as a CalibratedClassifierCV so I could get the class probabilities (predict_proba) using Platt scaling.\n","\n","I chose the feature set because it provides information about ngrams of various lengths, and filters too specific ngrams. I chose TF-IDF encoding to retain information about word frequency while normalizing the data based on document frequency. I chose SVC because it demonstrated much greater accuracy than NB, LR and K means, and it is supported by research as being a good choice for this task (https://arxiv.org/abs/2211.02563).\n","\n","The final AP performance is around 0.86."]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
